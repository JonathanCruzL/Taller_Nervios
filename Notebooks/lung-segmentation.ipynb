{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# FCNN (Fully Convolutional Neural Network)\n\nEste tipo de arquitecturas son conformadas por dos bloques principales, que conforman una trayectoria de codificador-decodificador o una trayectoria expansiva de contracción equivalente. En la figura se observan estas dos etapas y se muestra un ejemplo que cómo las características extraídas se hacen más particulares o especificas a medida que se profundiza en la red.\n\nLos codificadores-decodificadores pueden definirse así:\n\n* **Codificador (parte izquierda de la red)**: Codifica la imagen en una representación abstracta de las características de la imagen aplicando una secuencia de bloques convolucionales que disminuyen gradualmente la altura y la anchura de la representación, pero un número creciente de canales que corresponden a las características de la imagen.\n\n* **Decodificador (parte derecha de la red)**: Decodifica la representación de la imagen en una máscara binaria aplicando una secuencia de convoluciones ascendentes (NO es lo mismo que la deconvolución) que aumenta gradualmente la altura y la anchura de la representación hasta el tamaño de la imagen original y disminuye el número de canales hasta el número de clases que estamos segmentando.\n\n\n<div style=\"width:100%;text-align: center;\">\n<img src=\"https://i.imgur.com/WNCNVHS.png\" width=\"800\" height=\"400\"/>\n</div>\n\nLa FCNN es la arquitectura más utilizada para la segmentación de imágenes, entre ellas, se utiliza una en particular para imagenes médicas y se conoce como red U-net la cual tiene conexiones entre los dos bloques principales con el fin de aportar información a la etapa de reconstrucción del mapa de características.","metadata":{}},{"cell_type":"markdown","source":"# Objetivo de las redes U-Net\nU-Net es una arquitectura de red convolucional para la segmentación rápida y precisa de imágenes. La intención de U-Net es captar tanto las características del contexto como las de la ubicación. La idea principal de la aplicación es utilizar capas de contracción sucesivas, a las que siguen inmediatamente operadores de remuestreo para obtener salidas de mayor resolución en las imágenes de entrada. \n# Arquitectura UNET\n  \n\n<div>\n<img src=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/u-net-architecture.png\" width=\"900\" height=\"800\"/>\n</div>\n\n\nAl visualizar la forma de la arquitectura de la red, podemos ver por qué probablemente se llama U-Net. La arquitectura tiene forma de U, de ahí su nombre. La arquitectura de la red se ilustra en la figura anterior. Consta de una vía de contracción (lado izquierdo) y una vía de expansión (lado derecho) al igual que las FCNN y además contiene la conexión entre los dos bloque como se mencionó anteriormente. La ruta de contracción sigue la arquitectura típica de una red convolucional (CNN).\n\n","metadata":{}},{"cell_type":"markdown","source":"# Database para aplicar modelo U-net\nLa base de datos utilizada consiste en 800 imágenes de radiografía de tórax donde 394 imágenes corresponden a una manifestación de tuberculosis y 406 corresponden a radigrafías normales o de personas sanas. Esta base de datos es una combinación de dos bases de datos conocidas en el estado del arte para radiografía de tórax (Montgomery y Shenzen database).\n\n## Etapas de pre-procesamiento y entrenamiento\n1. Cargar la base de datos\n2. Preparación de los datos\n3. Imágenes de entrenamiento y de prueba\n4. Construyendo una red U-net\n5. Entrenamiento del modelo U-net\n6. Métricas y validacón","metadata":{}},{"cell_type":"markdown","source":"# Configuración","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport tensorflow as tf # Libreria para aprendizaje automático\nimport os\nimport PIL # Librería de edición de imágenes\nimport re \n\nfrom skimage import segmentation\nfrom glob import glob\nfrom collections import defaultdict # Define un diccionario\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme()\nsns.set(font_scale = 2)\n\nfrom ipywidgets import interact","metadata":{"execution":{"iopub.status.busy":"2022-10-26T05:51:48.698237Z","iopub.execute_input":"2022-10-26T05:51:48.698641Z","iopub.status.idle":"2022-10-26T05:51:54.056765Z","shell.execute_reply.started":"2022-10-26T05:51:48.698563Z","shell.execute_reply":"2022-10-26T05:51:54.055812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. Cargar la base de datos \n\nSobre los datos:\n* Hay una pequeña anormalidad en la convención de nombres de las máscaras.\n* Algunas imágenes no tienen sus correspondientes máscaras.\n\nComprobación de las radiografías y sus respectivas máscaras","metadata":{}},{"cell_type":"code","source":"# Declarando los directorios de las imágenes\nDIR = \"/kaggle/input/chest-xray-masks-and-labels/data/\"\n\nlung_image_paths = glob(os.path.join(DIR,\"Lung Segmentation/CXR_png/*.png\")) # Directorio de las imagenes radiográficas.\nmask_image_paths = glob(os.path.join(DIR,\"Lung Segmentation/masks/*.png\")) # Directorio de las máscaras, que indican la localización de pulmones.\n\nrelated_paths = defaultdict(list)\n\n# Combinando las imágenes de 1 para 1\nfor img_path in lung_image_paths:\n    img_match = re.search(\"CXR_png/(.*)\\.png$\", img_path)\n    if img_match:\n        img_name = img_match.group(1)\n    for mask_path in mask_image_paths:\n        mask_match = re.search(img_name, mask_path)\n        if mask_match:\n            related_paths[\"image_path\"].append(img_path)\n            related_paths[\"mask_path\"].append(mask_path)\n\npaths_df = pd.DataFrame.from_dict(related_paths)\npaths_df.head()","metadata":{"execution":{"iopub.status.busy":"2022-10-26T05:51:54.058763Z","iopub.execute_input":"2022-10-26T05:51:54.059435Z","iopub.status.idle":"2022-10-26T05:51:55.262006Z","shell.execute_reply.started":"2022-10-26T05:51:54.059396Z","shell.execute_reply":"2022-10-26T05:51:55.261119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Nimgs = paths_df.shape[0] # Número total de imagenes\n@interact(xray_num = (0,Nimgs-1,1))\ndef plot_pair_img_mask(xray_num):\n    img_path = paths_df[\"image_path\"][xray_num]\n    mask_path = paths_df[\"mask_path\"][xray_num]\n\n    img = PIL.Image.open(img_path)\n    mask = PIL.Image.open(mask_path)\n    \n## Visualizar imagen e independientemente su máscara\n#     fig = plt.figure(figsize = (10,10))\n\n#     ax1 = fig.add_subplot(2,2,1)\n#     ax1.imshow(img, cmap = \"gray\")\n#     plt.title(\"Original\");\n#     plt.axis('off')\n    \n#     ax2 = fig.add_subplot(2,2,2)\n#     plt.axis('off')\n#     ax2.imshow(mask, cmap = \"viridis\")\n#     plt.title(\"Máscara real\");\n#     fig.show()\n\n    plt.figure(figsize = (5,5))\n    plt.imshow(img, cmap = \"gray\")\n    edges_est = segmentation.clear_border(np.squeeze(mask))\n    plt.contour(edges_est,[0.5],colors=['red'])\n    plt.axis('off')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-26T05:51:55.266307Z","iopub.execute_input":"2022-10-26T05:51:55.268967Z","iopub.status.idle":"2022-10-26T05:51:57.285780Z","shell.execute_reply.started":"2022-10-26T05:51:55.268930Z","shell.execute_reply":"2022-10-26T05:51:57.284587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Preparación de los datos  \nEn este paso crearemos una función para tratar las imágenes de rayos X y las máscaras. Este proceso es necesario para poder preprocesar y normalizar las imágenes.  \nUtilizaremos la biblioteca `cv2` para redimensionar las imágenes y las máscaras. ","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm # librería para monitorear las funciones y sus tiempos de ejecución.\nimport cv2 # Librería Open-CV para python.\n\nimg_side_size = 256 # tamaño de las imágenes a procesar.\n\ndef prepare_train_test(df = pd.DataFrame(), resize_shape = tuple(), color_mode = \"rgb\"):\n    img_array = list()\n    mask_array = list()\n\n    # Preparando Imagenes\n    for image_path in tqdm(paths_df.image_path):\n        resized_image = cv2.resize(cv2.imread(image_path),resize_shape) # Reescalando las imagenes a una resolución dada por el parámetro resize_shape\n        resized_image = resized_image/255. # Normalizamos las intensidades de la imágen a valores entre 0 y 1. \n        # Procesar según el tipo de imagen (Color --> RGB | escala de grises --> Gray)\n        if color_mode == \"gray\":\n            img_array.append(resized_image[:,:,0])\n        elif color_mode == \"rgb\":\n            img_array.append(resized_image[:,:,:])\n            \n    # Preparando las mascaras\n    for mask_path in tqdm(paths_df.mask_path):\n        resized_mask = cv2.resize(cv2.imread(mask_path),resize_shape)\n        resized_mask = resized_mask/255.\n        mask_array.append(resized_mask[:,:,0])\n\n    return img_array, mask_array\n\nimg_array, mask_array = prepare_train_test(df = paths_df, resize_shape = (img_side_size,img_side_size), color_mode = \"gray\")","metadata":{"execution":{"iopub.status.busy":"2022-10-26T05:51:57.288889Z","iopub.execute_input":"2022-10-26T05:51:57.289304Z","iopub.status.idle":"2022-10-26T05:54:39.856284Z","shell.execute_reply.started":"2022-10-26T05:51:57.289264Z","shell.execute_reply":"2022-10-26T05:54:39.855199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Imágenes de entrenamiento y de prueba\nSeparación de los datos de entrenamiento y de prueba","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split # Función para separar el conjunto de entrenamiento y de test.\nimg_train, img_test, mask_train, mask_test = train_test_split(img_array, mask_array, test_size = 0.2, random_state= 42)\n\n# Se realiza un reshape para asegurar la forma del tensor adecuada: #img x W x H x C | donde W es el ancho de las imagenes, H es el alto y C el número de canales (gray C=1, RGB C=3)\n\nimg_side_size = 256\nimg_train = np.array(img_train).reshape(len(img_train), img_side_size, img_side_size, 1)\nimg_test = np.array(img_test).reshape(len(img_test), img_side_size, img_side_size, 1)\nmask_train = np.array(mask_train).reshape(len(mask_train), img_side_size, img_side_size, 1)\nmask_test = np.array(mask_test).reshape(len(mask_test), img_side_size, img_side_size, 1)","metadata":{"execution":{"iopub.status.busy":"2022-10-26T05:54:39.857624Z","iopub.execute_input":"2022-10-26T05:54:39.858640Z","iopub.status.idle":"2022-10-26T05:54:40.304405Z","shell.execute_reply.started":"2022-10-26T05:54:39.858602Z","shell.execute_reply":"2022-10-26T05:54:40.303381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Construyendo una red U-net\n\n* Las métricas utilizadas para evaluar los resultados son el coeficiente Dice e IoU (Intersection over Union):\n\n<div style=\"width:100%;text-align: center;\">\n<img src=\"https://www.researchgate.net/publication/328671987/figure/fig4/AS:688210103529478@1541093483784/Calculation-of-the-Dice-similarity-coefficient-The-deformed-contour-of-the-liver-from.ppm\" width=\"500\" height=\"300\"/>\n</div>\n\n<div style=\"width:100%;text-align: center;\">\n<img src=\"https://i.imgur.com/yJp0n0n.png\" width=\"500\" height=\"300\"/>\n</div>\n","metadata":{}},{"cell_type":"code","source":"# Función que calcula la medida Dice \ndef dice_coef(y_true, y_pred):\n    y_true_f = keras.flatten(y_true)\n    y_pred_f = keras.flatten(y_pred)\n    intersection = keras.sum(y_true_f * y_pred_f)\n    return (2. * intersection + 1) / (keras.sum(y_true_f) + keras.sum(y_pred_f) + 1)\n\n# Función que calcula la pérdida según el Dice.\ndef dice_coef_loss(y_true, y_pred):\n    return -dice_coef(y_true, y_pred)\n\n# Función Dice para test\ndef dice_coef_test(y_true, y_pred):\n    y_true_f = y_true.flatten()\n    y_pred_f = y_pred.flatten()\n    union = np.sum(y_true_f) + np.sum(y_pred_f)\n    if union==0: return 1\n    intersection = np.sum(y_true_f * y_pred_f)\n    return 2. * intersection / union\n\n# Intersection over Union metric\ndef IOU(y_true,y_pred):\n    intersection = np.sum(y_true * y_pred)\n    sum_ = np.sum(y_true) + np.sum(y_pred)\n    jac = (intersection + 1) / (sum_ - intersection + 1)\n    return jac","metadata":{"execution":{"iopub.status.busy":"2022-10-26T05:54:40.305973Z","iopub.execute_input":"2022-10-26T05:54:40.306366Z","iopub.status.idle":"2022-10-26T05:54:40.316176Z","shell.execute_reply.started":"2022-10-26T05:54:40.306328Z","shell.execute_reply":"2022-10-26T05:54:40.315039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* Conv2D: Para crear las capas de convolución 2D, utilizaremos los siguientes filtros = [32, 64, 256 y 512]. Esta capa crea un núcleo de convolución que ayuda a producir un tensor de salida.","metadata":{}},{"cell_type":"code","source":"from keras.models import *\nfrom keras.layers import *\nfrom keras.optimizers import *\nfrom keras import backend as keras\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler\nfrom tensorflow.keras.activations import *\nfrom tensorflow.keras.optimizers import Adam\n\n\ndef unet(input_size=(img_side_size,img_side_size,1)):\n    inputs = Input(input_size)\n    \"\"\"\n    codifica la imagen en una representación abstracta de las características de la imagen aplicando \n    una secuencia de bloques convolucionales que disminuyen gradualmente la altura y la anchura de la representación\n    \"\"\"\n    # codificador (parte izquierda de la “U”)\n    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n\n    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n\n    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n\n    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n\n    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(pool4)\n    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n    \"\"\"\n    decodifica la representación de la imagen en una máscara binaria aplicando una secuencia de convoluciones ascendentes.\n    que aumenta gradualmente la altura y la anchura de la representación hasta el tamaño de la imagen original y disminuye el número de \n    de canales al número de clases que estamos segmentando\n    \"\"\"\n    # decodificador (parte derecha de la “U”)\n    up6 = concatenate([Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5), conv4], axis=3)\n    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n\n    up7 = concatenate([Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6), conv3], axis=3)\n    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n\n    up8 = concatenate([Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7), conv2], axis=3)\n    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n\n    up9 = concatenate([Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8), conv1], axis=3)\n    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n    \n    #\n    conv10 = Conv2D(1, (1, 1), activation='sigmoid')(conv9)\n\n    return Model(inputs=[inputs], outputs=[conv10])","metadata":{"execution":{"iopub.status.busy":"2022-10-26T05:54:40.317735Z","iopub.execute_input":"2022-10-26T05:54:40.318341Z","iopub.status.idle":"2022-10-26T05:54:41.099370Z","shell.execute_reply.started":"2022-10-26T05:54:40.318304Z","shell.execute_reply":"2022-10-26T05:54:41.098358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 10\ninput_size=(img_side_size,img_side_size,1)\n\nmodel = unet(input_size=(img_side_size,img_side_size,1)) # Creamos el modelo \nmodel.compile( # Opciones de compilación\n    optimizer=Adam(learning_rate=5*1e-4), \n    loss=\"binary_crossentropy\",\n    metrics=[dice_coef, 'binary_accuracy']\n)\n\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-10-26T05:54:41.100703Z","iopub.execute_input":"2022-10-26T05:54:41.101101Z","iopub.status.idle":"2022-10-26T05:54:44.105825Z","shell.execute_reply.started":"2022-10-26T05:54:41.101065Z","shell.execute_reply":"2022-10-26T05:54:44.104740Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.utils import plot_model\nplot_model(model,\n           to_file=\"model.png\",\n           show_shapes=True,\n           show_layer_names=True)","metadata":{"execution":{"iopub.status.busy":"2022-10-26T05:54:44.107386Z","iopub.execute_input":"2022-10-26T05:54:44.107731Z","iopub.status.idle":"2022-10-26T05:54:45.372375Z","shell.execute_reply.started":"2022-10-26T05:54:44.107696Z","shell.execute_reply":"2022-10-26T05:54:45.370558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Entrenamiento del modelo U-Net\n\nPara nuestro entrenamiento utilizaremos `epochs = 30`, que es el número de veces que recorreremos el conjunto de lotes de entrenamiento.","metadata":{}},{"cell_type":"code","source":"earlystopping = tf.keras.callbacks.EarlyStopping(\n    monitor='loss', \n    patience=10\n) # Callback, si después de 10 epocas, la pérdida no se reduce o cambia. Se para el entrenamiento de forma temprana.\n\nhistory = model.fit(\n    x = img_train, \n    y = mask_train, \n    validation_data = (img_test, mask_test), \n    epochs = 30,\n    batch_size = 16,\n    callbacks = [earlystopping]\n)","metadata":{"execution":{"iopub.status.busy":"2022-10-26T05:54:45.378159Z","iopub.execute_input":"2022-10-26T05:54:45.379364Z","iopub.status.idle":"2022-10-26T05:58:09.673426Z","shell.execute_reply.started":"2022-10-26T05:54:45.379321Z","shell.execute_reply":"2022-10-26T05:58:09.672175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Guardado del modelo en la salida","metadata":{}},{"cell_type":"code","source":"model.save('./model_seg.h5')","metadata":{"execution":{"iopub.status.busy":"2022-10-26T05:58:09.702952Z","iopub.execute_input":"2022-10-26T05:58:09.703783Z","iopub.status.idle":"2022-10-26T05:58:09.988105Z","shell.execute_reply.started":"2022-10-26T05:58:09.703746Z","shell.execute_reply":"2022-10-26T05:58:09.987105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Visualización del resultado\nAquí crearemos una función para visualizar la imagen de vista previa, la imagen de la máscara real, la superposición sobre la máscara real y la imagen original.","metadata":{}},{"cell_type":"code","source":"Nimgs = img_test.shape[0] # Número total de imagenes\nimg_array = img_test\n@interact(img_num = (0,Nimgs-1,1))\ndef test_on_image( img_num ):\n    \"\"\" para datos de previsión >= .5 asignaremos 1, en caso contrario el valor será 0 # 6. métricas y validación  \n**Entropía cruzada (pérdida)**: para cuantificar la diferencia entre las dos distribuciones de probabilidad (entrenamiento y validación).\n* **Coeficiente de los dados**: El coeficiente de Dice es un estadístico utilizado para medir la similitud de dos muestras, una de las métricas más utilizadas en el contexto de la segmentación de imágenes.\"\"\"\n    pred = model.predict(img_array[img_num].reshape(1,img_side_size,img_side_size,1))\n    pred[pred>0.5] = 1.0\n    pred[pred<0.5] = 0.0\n    \n    dice = dice_coef_test(y_true = mask_test[img_num], y_pred = pred)\n    iou = IOU(y_true = mask_test[img_num], y_pred = pred)\n    fig = plt.figure(figsize = (15,10))\n    \n    plt.subplot(1,3,1)\n    plt.imshow(pred.reshape(img_side_size, img_side_size), cmap = \"PuBu\")\n    plt.title(\"Predicción\")\n    plt.axis(\"off\")\n    \n    plt.subplot(1,3,2)\n    plt.imshow(mask_test[img_num].reshape(img_side_size, img_side_size), cmap = \"viridis\")\n    plt.title(\"Máscara real\");\n    plt.axis(\"off\")\n    \n    plt.subplot(1,3,3)\n#     plt.figure(figsize = (5,5))\n    plt.imshow(img_array[img_num].reshape(img_side_size, img_side_size), cmap = \"gray\")\n    edges_est1 = segmentation.clear_border(np.squeeze(mask_test[img_num].reshape(img_side_size, img_side_size)))\n    edges_est2 = segmentation.clear_border(np.squeeze(pred.reshape(img_side_size, img_side_size)))\n    plt.contour(edges_est1,[0.5],colors=['red'])\n    plt.contour(edges_est2,[0.5],colors=['blue'])\n    \n    plt.title('Superposición')\n    plt.axis('off')\n#     plt.savefig('./prediction.png', bbox_inches='tight', pad_inches=0)\n    plt.suptitle('Dice: {:2.2%} and IOU: {:2.2%}'.format(dice,iou))\n    plt.show()\n    \n    \n#     plt.imshow(img_array[img_num].reshape(img_side_size, img_side_size), cmap = \"gray\")\n#     plt.imshow(mask_test[img_num].reshape(img_side_size, img_side_size), cmap = \"viridis\", alpha = 0.5)\n#     plt.imshow(pred.reshape(img_side_size, img_side_size),cmap = \"PuBu\", alpha = 0.3)\n#     plt.title(\"Superposición\")\n#     plt.axis(\"off\")\n    \n#     plt.subplot(1,4,4)\n#     plt.imshow(img_array[img_num].reshape(img_side_size, img_side_size), cmap = \"gray\")\n#     plt.title(\"Original\")\n#     plt.axis(\"off\")\n    \n    return None\n","metadata":{"execution":{"iopub.status.busy":"2022-10-26T06:08:46.409045Z","iopub.execute_input":"2022-10-26T06:08:46.409422Z","iopub.status.idle":"2022-10-26T06:08:46.812395Z","shell.execute_reply.started":"2022-10-26T06:08:46.409391Z","shell.execute_reply":"2022-10-26T06:08:46.811443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Métricas y validación  \n+ **Cross-entropy (pérdida)**: para cuantificar la diferencia entre las dos distribuciones de probabilidad (entrenamiento y validación).\n* **Coeficiente Dice**: El coeficiente de Dice es un estadístico utilizado para medir la similitud de dos muestras, una de las métricas más utilizadas en el contexto de la segmentación de imágenes.","metadata":{}},{"cell_type":"code","source":"def get_metrics(history):\n    fig = plt.figure(figsize = (15,15))\n    plt.subplot(2,2,1)\n    plt.plot(history.history[\"loss\"], label = \"loss entrenamiento\")\n    plt.plot(history.history[\"val_loss\"], label = \"loss validación\")\n    plt.title(\"Validación x Entrenamiento: Entropía binaria\", fontsize=18, y=1)\n    plt.legend()\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Entropia binaria\")\n\n    plt.subplot(2,2,2)\n    plt.plot(history.history[\"dice_coef\"], label = \"Coeficiente Dice entrenameinto\")\n    plt.plot(history.history[\"val_dice_coef\"], label = \"Coeficiente Dice validación\")\n    plt.title(\"Validación x Entrenamiento: Coeficiente Dice\", fontsize=18, y=1)\n    plt.legend()\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Coeficiente Dice\")\n    \nget_metrics(history = history)","metadata":{"execution":{"iopub.status.busy":"2022-10-26T06:15:47.856073Z","iopub.execute_input":"2022-10-26T06:15:47.856526Z","iopub.status.idle":"2022-10-26T06:15:48.304680Z","shell.execute_reply.started":"2022-10-26T06:15:47.856491Z","shell.execute_reply":"2022-10-26T06:15:48.303693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Comprobación de la validación de la prueba mediante el coeficiente Dice  \nUtilizaremos los datos de la prueba para hacer nuestra predicción y realizaremos el solapamiento entre la máscara predicha y la real. Se examinarán 141 imágenes.","metadata":{}},{"cell_type":"code","source":"dice_coefs = list()\npred = model.predict(img_test)\npred[pred>=0.5] = 1\npred[pred<0.5] = 0\nfor i in tqdm(range(len(img_test))):\n    prediction = pred[i]\n    dice = dice_coef_test(y_true = mask_test[i], y_pred = prediction)\n    dice_coefs.append(dice)","metadata":{"execution":{"iopub.status.busy":"2022-10-26T06:15:51.738775Z","iopub.execute_input":"2022-10-26T06:15:51.739274Z","iopub.status.idle":"2022-10-26T06:15:55.163105Z","shell.execute_reply.started":"2022-10-26T06:15:51.739228Z","shell.execute_reply":"2022-10-26T06:15:55.161526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize =(10, 7))\nplt.hist(np.array(dice_coefs), bins = 50)\nplt.title(\"Distribución del Coeficiente Dice (141 imágenes)\",fontsize=20, y=1)\nplt.xlabel(\"Coeficiente Dice\");\nplt.ylabel(\"Número de imágenes\");\nprint(f'Mediana Coef. Dice: {np.median(np.array(dice_coefs))}');\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-26T06:15:57.232184Z","iopub.execute_input":"2022-10-26T06:15:57.232539Z","iopub.status.idle":"2022-10-26T06:15:57.510653Z","shell.execute_reply.started":"2022-10-26T06:15:57.232508Z","shell.execute_reply":"2022-10-26T06:15:57.509706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize =(10, 7))\nplt.boxplot(np.array(dice_coefs))\nplt.title(\"Distribución del Coeficiente Dice (141 imágenes)\",fontsize=20, y=1)\nplt.ylabel(\"Coeficiente Dice\");\nprint(f'Mediana Coef. Dice: {np.median(np.array(dice_coefs))}');\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-10-26T06:16:04.419735Z","iopub.execute_input":"2022-10-26T06:16:04.420426Z","iopub.status.idle":"2022-10-26T06:16:04.600150Z","shell.execute_reply.started":"2022-10-26T06:16:04.420389Z","shell.execute_reply":"2022-10-26T06:16:04.599132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* ¿Qué sucede si la pérdida no es la Cross-entropy, sino el coeficiente Dice?\n* ¿Lo anterior es posible?","metadata":{}}]}